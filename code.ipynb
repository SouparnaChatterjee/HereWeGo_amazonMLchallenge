{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sM7VXiPkowQ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f234fb08-b6ed-4e6c-b470-46faa12acc6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m119.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m723.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.9/295.9 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "Loading source datasets...\n",
            "Engineering features for all data...\n",
            "Training set size: 60000\n",
            "Validation set size: 15000\n",
            "Vectorizing text with TF-IDF using top 2200 features...\n",
            "Final Training Matrix Shape: (60000, 2203)\n",
            "Final Validation Matrix Shape: (15000, 2203)\n",
            "Training CatBoost Regressor...\n",
            "0:\tlearn: 0.9346760\ttest: 0.9306880\tbest: 0.9306880 (0)\ttotal: 209ms\tremaining: 8m 42s\n",
            "500:\tlearn: 0.7056399\ttest: 0.7273493\tbest: 0.7273493 (500)\ttotal: 1m 5s\tremaining: 4m 23s\n",
            "1000:\tlearn: 0.6576568\ttest: 0.7091978\tbest: 0.7091978 (1000)\ttotal: 2m 11s\tremaining: 3m 16s\n",
            "1500:\tlearn: 0.6243853\ttest: 0.7012589\tbest: 0.7012588 (1499)\ttotal: 3m 16s\tremaining: 2m 10s\n",
            "2000:\tlearn: 0.5975798\ttest: 0.6967317\tbest: 0.6967317 (2000)\ttotal: 4m 20s\tremaining: 1m 5s\n",
            "2499:\tlearn: 0.5735310\ttest: 0.6932148\tbest: 0.6932118 (2498)\ttotal: 5m 25s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.693211811\n",
            "bestIteration = 2498\n",
            "\n",
            "Shrink model to first 2499 iterations.\n",
            "CatBoost training complete.\n",
            "\n",
            "Training LightGBM Regressor...\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.221843 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 480240\n",
            "[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 2203\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Start training from score 2.739574\n",
            "[500]\tvalid_0's l2: 0.498661\n",
            "[1000]\tvalid_0's l2: 0.487172\n",
            "[1500]\tvalid_0's l2: 0.482828\n",
            "[2000]\tvalid_0's l2: 0.480967\n",
            "LightGBM training complete.\n",
            "\n",
            "Training XGBoost Regressor...\n",
            "[0]\tvalidation_0-rmse:0.92920\n",
            "[500]\tvalidation_0-rmse:0.71199\n",
            "[1000]\tvalidation_0-rmse:0.70208\n",
            "[1500]\tvalidation_0-rmse:0.69817\n",
            "[2000]\tvalidation_0-rmse:0.69634\n",
            "[2499]\tvalidation_0-rmse:0.69528\n",
            "XGBoost training complete.\n",
            "\n",
            "--- Performance Evaluation on Validation Set ---\n",
            "\n",
            "VALIDATION SMAPE SCORE: 52.7771%\n",
            "-------------------------------------------------\n",
            "\n",
            "Generating predictions for the final submission...\n",
            "\n",
            "Creating final submission file...\n",
            "Submission file saved to: /content/drive/MyDrive/ML_Challenge_2025/submissions/submission_3-gbdt_ensemble_with_val.csv\n",
            "   sample_id      price\n",
            "0     100179  15.922215\n",
            "1     245611  13.379216\n",
            "2     146263  19.765680\n",
            "3      95658  13.473074\n",
            "4      36806  56.066406\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "#                 ROBUST 3-GBDT PRICE FORECASTER (v2 - WITH SMAPE)\n",
        "#\n",
        "#   Author: [Your Name]\n",
        "#   Strategy: An ensemble of three GBDT models. This version includes a\n",
        "#             train-validation split to calculate a reliable SMAPE score before submission.\n",
        "# ==============================================================================\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 1. Environment Setup & Initializations\n",
        "# Install and import necessary libraries for our forecasting task.\n",
        "\n",
        "# %%\n",
        "!pip install catboost lightgbm xgboost scikit-learn -q\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import gc # Memory Management\n",
        "import lightgbm as lgb\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# --- Model Imports ---\n",
        "from catboost import CatBoostRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# --- Mount Drive and Configure Paths ---\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "BASE_PATH = \"/content/drive/MyDrive/ML_Challenge_2025\"\n",
        "DATA_PATH = os.path.join(BASE_PATH, \"data\")\n",
        "SUBMISSIONS_PATH = os.path.join(BASE_PATH, \"submissions\")\n",
        "os.makedirs(SUBMISSIONS_PATH, exist_ok=True)\n",
        "\n",
        "# --- Configuration ---\n",
        "RANDOM_SEED = 2024\n",
        "N_FEATURES_TFIDF = 2200\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 2. SMAPE Metric and Feature Engineering\n",
        "\n",
        "# %%\n",
        "def smape(y_true, y_pred):\n",
        "    \"\"\"Calculates the Symmetric Mean Absolute Percentage Error (SMAPE).\"\"\"\n",
        "    numerator = np.abs(y_pred - y_true)\n",
        "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
        "    # Add a small epsilon to the denominator to avoid division by zero\n",
        "    return np.mean(numerator / (denominator + 1e-9)) * 100\n",
        "\n",
        "def create_features(df):\n",
        "    processed_df = df.copy()\n",
        "    processed_df['catalog_content'] = processed_df['catalog_content'].fillna('missing')\n",
        "    def get_ipq(text):\n",
        "        match = re.search(r'(?:pack of|set of|pk of|of|x|\\s)(\\d{1,3})', text.lower())\n",
        "        if match: num = int(match.group(1)); return num if 1 < num <= 100 else 1\n",
        "        return 1\n",
        "    processed_df['pack_quantity'] = processed_df['catalog_content'].apply(get_ipq)\n",
        "    processed_df['content_length_chars'] = processed_df['catalog_content'].str.len()\n",
        "    processed_df['content_word_count'] = processed_df['catalog_content'].str.split().str.len()\n",
        "    return processed_df\n",
        "\n",
        "print(\"Loading source datasets...\")\n",
        "source_train_df_full = pd.read_csv(f\"{DATA_PATH}/train.csv\")\n",
        "source_test_df = pd.read_csv(f\"{DATA_PATH}/test.csv\")\n",
        "\n",
        "print(\"Engineering features for all data...\")\n",
        "train_featured_full = create_features(source_train_df_full)\n",
        "test_featured = create_features(source_test_df)\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 3. Train-Validation Split\n",
        "# We split the full training data to create a validation set for reliable performance evaluation.\n",
        "\n",
        "# %%\n",
        "# Split the featured data into training (80%) and validation (20%) sets\n",
        "train_featured, val_featured = train_test_split(\n",
        "    train_featured_full,\n",
        "    test_size=0.2,\n",
        "    random_state=RANDOM_SEED\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {len(train_featured)}\")\n",
        "print(f\"Validation set size: {len(val_featured)}\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 4. Text-to-Vector Transformation\n",
        "# The TF-IDF vectorizer is now FIT ONLY on the new training split to prevent data leakage.\n",
        "\n",
        "# %%\n",
        "print(f\"Vectorizing text with TF-IDF using top {N_FEATURES_TFIDF} features...\")\n",
        "tfidf_vec = TfidfVectorizer(max_features=N_FEATURES_TFIDF, stop_words='english', ngram_range=(1, 2))\n",
        "\n",
        "# Fit on the training split, then transform all three sets\n",
        "train_text_features = tfidf_vec.fit_transform(train_featured['catalog_content'])\n",
        "val_text_features = tfidf_vec.transform(val_featured['catalog_content'])\n",
        "test_text_features = tfidf_vec.transform(test_featured['catalog_content'])\n",
        "\n",
        "tfidf_train = pd.DataFrame(train_text_features.toarray(), columns=tfidf_vec.get_feature_names_out())\n",
        "tfidf_val = pd.DataFrame(val_text_features.toarray(), columns=tfidf_vec.get_feature_names_out())\n",
        "tfidf_test = pd.DataFrame(test_text_features.toarray(), columns=tfidf_vec.get_feature_names_out())\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 5. Final Feature Assembly\n",
        "\n",
        "# %%\n",
        "numerical_cols = ['pack_quantity', 'content_length_chars', 'content_word_count']\n",
        "\n",
        "# Create final matrices for train, validation, and test sets\n",
        "X_train = pd.concat([train_featured[numerical_cols].reset_index(drop=True), tfidf_train], axis=1)\n",
        "X_val = pd.concat([val_featured[numerical_cols].reset_index(drop=True), tfidf_val], axis=1)\n",
        "X_test = pd.concat([test_featured[numerical_cols].reset_index(drop=True), tfidf_test], axis=1)\n",
        "\n",
        "# Create log-transformed target variables for train and validation\n",
        "y_train_log = np.log1p(train_featured['price'])\n",
        "y_val_log = np.log1p(val_featured['price'])\n",
        "\n",
        "print(f\"Final Training Matrix Shape: {X_train.shape}\")\n",
        "print(f\"Final Validation Matrix Shape: {X_val.shape}\")\n",
        "del train_featured_full, train_featured, val_featured, tfidf_train, tfidf_val, tfidf_test\n",
        "gc.collect()\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 6. Model Training Pipeline\n",
        "\n",
        "# %%\n",
        "def train_all_models(X_tr, y_tr, X_vl, y_vl):\n",
        "    models = {}\n",
        "    print(\"Training CatBoost Regressor...\"); model_catboost = CatBoostRegressor(iterations=2500, learning_rate=0.045, depth=8, loss_function='RMSE', random_seed=RANDOM_SEED, verbose=500, early_stopping_rounds=100); model_catboost.fit(X_tr, y_tr, eval_set=(X_vl, y_vl)); models['catboost'] = model_catboost; print(\"CatBoost training complete.\")\n",
        "    print(\"\\nTraining LightGBM Regressor...\"); model_lightgbm = LGBMRegressor(n_estimators=2500, learning_rate=0.045, num_leaves=31, random_state=RANDOM_SEED, n_jobs=-1); model_lightgbm.fit(X_tr, y_tr, eval_set=[(X_vl, y_vl)], callbacks=[lgb.early_stopping(100, verbose=False), lgb.log_evaluation(period=500)]); models['lightgbm'] = model_lightgbm; print(\"LightGBM training complete.\")\n",
        "    X_tr_xgb = X_tr.copy(); X_tr_xgb.columns = [\"\".join(c if c.isalnum() else \"_\" for c in str(x)) for x in X_tr_xgb.columns]\n",
        "    X_vl_xgb = X_vl.copy(); X_vl_xgb.columns = [\"\".join(c if c.isalnum() else \"_\" for c in str(x)) for x in X_vl_xgb.columns]\n",
        "    print(\"\\nTraining XGBoost Regressor...\"); model_xgboost = XGBRegressor(n_estimators=2500, learning_rate=0.045, max_depth=7, objective='reg:squarederror', random_state=RANDOM_SEED, n_jobs=-1, eval_metric='rmse', early_stopping_rounds=100); model_xgboost.fit(X_tr_xgb, y_tr, eval_set=[(X_vl_xgb, y_vl)], verbose=500); models['xgboost'] = model_xgboost; print(\"XGBoost training complete.\")\n",
        "    return models\n",
        "\n",
        "trained_models = train_all_models(X_train, y_train_log, X_val, y_val_log)\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 7. Validation Performance and Prediction\n",
        "\n",
        "# %%\n",
        "print(\"\\n--- Performance Evaluation on Validation Set ---\")\n",
        "\n",
        "# Predict on the validation set\n",
        "preds_val_log_cat = trained_models['catboost'].predict(X_val)\n",
        "preds_val_log_lgb = trained_models['lightgbm'].predict(X_val)\n",
        "X_val_xgb = X_val.copy(); X_val_xgb.columns = [\"\".join(c if c.isalnum() else \"_\" for c in str(x)) for x in X_val_xgb.columns]\n",
        "preds_val_log_xgb = trained_models['xgboost'].predict(X_val_xgb)\n",
        "\n",
        "# Ensemble the validation predictions\n",
        "ensemble_preds_val_log = np.mean([preds_val_log_cat, preds_val_log_lgb, preds_val_log_xgb], axis=0)\n",
        "\n",
        "# Convert predictions and true values back to original price scale\n",
        "y_val_true_price = np.expm1(y_val_log)\n",
        "preds_val_price = np.expm1(ensemble_preds_val_log)\n",
        "\n",
        "# Calculate and print the SMAPE score\n",
        "validation_smape = smape(y_val_true_price, preds_val_price)\n",
        "print(f\"\\nVALIDATION SMAPE SCORE: {validation_smape:.4f}%\")\n",
        "print(\"-------------------------------------------------\")\n",
        "\n",
        "# --- Generate Predictions for the Test Set ---\n",
        "print(\"\\nGenerating predictions for the final submission...\")\n",
        "preds_test_log_cat = trained_models['catboost'].predict(X_test)\n",
        "preds_test_log_lgb = trained_models['lightgbm'].predict(X_test)\n",
        "X_test_xgb = X_test.copy(); X_test_xgb.columns = [\"\".join(c if c.isalnum() else \"_\" for c in str(x)) for x in X_test_xgb.columns]\n",
        "preds_test_log_xgb = trained_models['xgboost'].predict(X_test_xgb)\n",
        "\n",
        "ensemble_preds_test_log = np.mean([preds_test_log_cat, preds_test_log_lgb, preds_test_log_xgb], axis=0)\n",
        "final_price_predictions = np.expm1(ensemble_preds_test_log)\n",
        "final_price_predictions[final_price_predictions < 0] = 0.01\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 8. Submission File Generation\n",
        "\n",
        "# %%\n",
        "print(\"\\nCreating final submission file...\")\n",
        "submission_df = pd.DataFrame({'sample_id': source_test_df['sample_id'], 'price': final_price_predictions})\n",
        "submission_path = os.path.join(SUBMISSIONS_PATH, 'submission_3-gbdt_ensemble_with_val.csv')\n",
        "submission_df.to_csv(submission_path, index=False)\n",
        "print(f\"Submission file saved to: {submission_path}\")\n",
        "print(submission_df.head())"
      ]
    }
  ]
}